# FlanSummit
üöÄ FlanSummit: Dialogue Summarization with FLAN-T5, PEFT Fine-tuning, RLHF, and Model Optimization

**FlanSummit** is an end-to-end applied NLP project that demonstrates how to take an open-source LLM and progressively enhance it for a specialized task ‚Äî dialogue summarization ‚Äî through efficient fine-tuning, reinforcement learning, compression, and deployment strategies.

---

## üéØ Motivation
- Apply **Parameter-Efficient Fine-Tuning (PEFT/LoRA)** to adapt large models with minimal compute.
- Leverage **Reinforcement Learning with Human Feedback (RLHF)** to align model outputs more closely with task-specific rewards.
- Explore **quantization techniques** to optimize the model for production use.
- Perform **distillation** to create lightweight versions of the model for edge deployment.
- Build a **full-stack solution** from training to evaluation to deployment.

---

## üìö Project Features
- **Base Model:** FLAN-T5-BASE (starting from SMALL to XXL) is an open source LLM first developed by Google using multi task instruction fine-tuning of T5 using FLAN dataset. Refer [this paper.](https://arxiv.org/pdf/2210.11416)

- **Dataset:**  
  - [DialogSum](https://github.com/cylnlp/dialogsum): A Real-life Scenario Dialogue Summarization Dataset.

- **Fine-tuning:** PEFT/LoRA for efficient parameter updates.

- **RLHF:** Reward Model + PPO fine-tuning via HuggingFace `trl`.

- **Quantization:** Exploring Post-Training Quantization (PTQ) and Quantization Aware Training (QAT).

- **Distillation:** Training a smaller student model using logits/soft targets considering the already trained model as the teacher.

- **Evaluation:** ROUGE-1, ROUGE-2, ROUGE-L metrics

---

## üõ†Ô∏è Tech Stack

| Component | Library/Tool |
|:---|:---|
| Model Backbone | HuggingFace Transformers |
| Fine-Tuning | PEFT |
| Reinforcement Learning | TRL |
| Quantization | bitsandbytes, Optimum |
| Distillation | HuggingFace Trainer |
| Evaluation | HuggingFace `evaluate`, ROUGE |
| Deployment (optional) | <TBD Gradio/Streamlit, Docker> |


## üìÅ Project Structure
```
FlanSummit/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ load_dataset.py
‚îÇ   ‚îî‚îÄ‚îÄ preprocess.py
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ baseline_inference.py
‚îÇ   ‚îú‚îÄ‚îÄ train_peft.py
‚îÇ   ‚îú‚îÄ‚îÄ train_reward_model.py
‚îÇ   ‚îú‚îÄ‚îÄ rl_training.py
‚îÇ   ‚îú‚îÄ‚îÄ quantization.py
‚îÇ   ‚îú‚îÄ‚îÄ distillation.py
‚îÇ   ‚îî‚îÄ‚îÄ evaluation.py
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ peft_config.json
‚îÇ   ‚îú‚îÄ‚îÄ rl_config.json
‚îÇ   ‚îî‚îÄ‚îÄ quantization_config.json
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îî‚îÄ‚îÄ evaluation_metrics.json
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ setup.sh
```

## ‚ö° Quickstart Guide

### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/flansummit.git
cd flansummit
```

### 2. Install Requirements
```bash
pip install -r requirements.txt
```

### 3. Prepare Dataset
```bash
python data/load_dataset.py
```

---

## üß™ Training and Evaluation Steps

### ‚û°Ô∏è Step 1: Baseline Evaluation
```bash
python src/baseline_inference.py
```

### ‚û°Ô∏è Step 2: Fine-tuning with PEFT/LoRA
```bash
python src/train_peft.py --config config/peft_config.json
```

### ‚û°Ô∏è Step 3: Reward Model Training
```bash
python src/train_reward_model.py
```

### ‚û°Ô∏è Step 4: RLHF Fine-tuning with PPO
```bash
python src/rl_training.py --config config/rl_config.json
```

### ‚û°Ô∏è Step 5: Quantization (PTQ/QAT)
```bash
python src/quantization.py --quant_type ptq
```

### ‚û°Ô∏è Step 6: Distillation to a Smaller Model
```bash
python src/distillation.py
```

### ‚û°Ô∏è Step 7: Final Evaluation
```bash
python src/evaluation.py
```

---

## üìä Results

TBD

---

---

## üìä Architecture Flow

```mermaid
flowchart TD
    A([üöÄ Load FLAN-T5]) --> B([üß™ Baseline Evaluation])
    A --> C([üîß Fine-tune with PEFT/LoRA])
    C --> D([üèÜ Train Reward Model])
    D --> E([üéØ RLHF Fine-tuning with PPO])
    E --> F([üß© Quantization - PTQ or QAT])
    E --> G([üå± Distillation to Small Model])
    F --> H([üö¢ Deployment Ready Model])
    G --> H

    style A fill:#e0f7fa,stroke:#26c6da,stroke-width:2px
    style B fill:#fff9c4,stroke:#fbc02d,stroke-width:2px
    style C fill:#dcedc8,stroke:#8bc34a,stroke-width:2px
    style D fill:#ffe0b2,stroke:#ff9800,stroke-width:2px
    style E fill:#f8bbd0,stroke:#e91e63,stroke-width:2px
    style F fill:#c5cae9,stroke:#3f51b5,stroke-width:2px
    style G fill:#d1c4e9,stroke:#7e57c2,stroke-width:2px
    style H fill:#b2ebf2,stroke:#00acc1,stroke-width:3px
```

---

## üßê About the Author
[Bincy Narath](https://www.linkedin.com/in/bincynarath/)
